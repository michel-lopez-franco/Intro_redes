{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El score (puntuación)\n",
    "\n",
    "El score (puntuación) es una medida de rendimiento que evalúa qué tan bien está funcionando un modelo. Hay diferentes tipos de scores dependiendo del tipo de problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Score (Precisión)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "# Mide el porcentaje de predicciones correctas\n",
    "\n",
    "# F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "# Media armónica entre precisión y recall\n",
    "\n",
    "# Precision Score\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_true, y_pred)\n",
    "# Proporción de verdaderos positivos sobre total de positivos predichos\n",
    "\n",
    "# Recall Score\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_true, y_pred)\n",
    "# Proporción de verdaderos positivos sobre total de positivos reales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R² Score (Coeficiente de determinación)\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "# Mide qué tan bien el modelo se ajusta a los datos\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "# Promedio de los errores al cuadrado\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "# Promedio de los errores absolutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "MSE: 0.0250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluar_modelo(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula múltiples métricas de evaluación\n",
    "    \"\"\"\n",
    "    # Convertir probabilidades a clases (para clasificación binaria)\n",
    "    y_pred_class = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_true, y_pred_class)\n",
    "    precision = precision_score(y_true, y_pred_class)\n",
    "    recall = recall_score(y_true, y_pred_class)\n",
    "    f1 = f1_score(y_true, y_pred_class)\n",
    "    \n",
    "    # Calcular error cuadrático medio\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'mse': mse\n",
    "    }\n",
    "\n",
    "# Ejemplo de uso\n",
    "y_true = np.array([0, 1, 1, 0])  # Valores reales\n",
    "y_pred = np.array([0.1, 0.9, 0.8, 0.2])  # Predicciones del modelo\n",
    "\n",
    "scores = evaluar_modelo(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de las métricas:\n",
    "\n",
    "```markdown\n",
    "**Accuracy (Precisión):** Mide el porcentaje de predicciones correctas realizadas por el modelo.\n",
    "\n",
    "Rango: 0 a 1\n",
    "1 es perfecto\n",
    "Útil para clases balanceadas\n",
    "\n",
    "\n",
    "**Precision (Precisión Positiva):** Proporción de verdaderos positivos sobre el total de positivos predichos:\n",
    "\n",
    "Rango: 0 a 1\n",
    "Mide falsos positivos\n",
    "Importante cuando el costo de falsos positivos es alto\n",
    "\n",
    "\n",
    "Recall (Sensibilidad):\n",
    "\n",
    "Rango: 0 a 1\n",
    "Mide falsos negativos\n",
    "Importante cuando el costo de falsos negativos es alto\n",
    "\n",
    "\n",
    "F1-Score:\n",
    "\n",
    "Rango: 0 a 1\n",
    "Balance entre precision y recall\n",
    "Útil para clases desbalanceadas\n",
    "\n",
    "\n",
    "R²:\n",
    "\n",
    "Rango: -∞ a 1\n",
    "1 es ajuste perfecto\n",
    "Útil para regresión\n",
    "\n",
    "\n",
    "MSE:\n",
    "\n",
    "Rango: 0 a ∞\n",
    "0 es perfecto\n",
    "Penaliza errores grandes\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo con validación cruzada:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def evaluar_con_cv(modelo, X, y, cv=5):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo usando validación cruzada\n",
    "    \"\"\"\n",
    "    # Definir el evaluador de pliegues\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Calcular scores para diferentes métricas\n",
    "    accuracy_scores = cross_val_score(modelo, X, y, cv=kf, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(modelo, X, y, cv=kf, scoring='precision')\n",
    "    recall_scores = cross_val_score(modelo, X, y, cv=kf, scoring='recall')\n",
    "    f1_scores = cross_val_score(modelo, X, y, cv=kf, scoring='f1')\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"Accuracy: {accuracy_scores.mean():.4f} (+/- {accuracy_scores.std()*2:.4f})\")\n",
    "    print(f\"Precision: {precision_scores.mean():.4f} (+/- {precision_scores.std()*2:.4f})\")\n",
    "    print(f\"Recall: {recall_scores.mean():.4f} (+/- {recall_scores.std()*2:.4f})\")\n",
    "    print(f\"F1-Score: {f1_scores.mean():.4f} (+/- {f1_scores.std()*2:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_scores,\n",
    "        'precision': precision_scores,\n",
    "        'recall': recall_scores,\n",
    "        'f1': f1_scores\n",
    "    }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecuaciones principales de las métricas más utilizadas:\n",
    "\n",
    "### Clasificación Binaria - Métricas Básicas:\n",
    "\n",
    "```markdown\n",
    "TP = True Positives (Verdaderos Positivos)\n",
    "TN = True Negatives (Verdaderos Negativos)\n",
    "FP = False Positives (Falsos Positivos)\n",
    "FN = False Negatives (Falsos Negativos)\n",
    "```\n",
    "\n",
    "```math\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} &= \\frac{TP + TN}{TP + TN + FP + FN} \\\\\n",
    "\\text{Precision} &= \\frac{TP}{TP + FP} \\\\\n",
    "\\text{Recall} &= \\frac{TP}{TP + FN}\\\\\n",
    "\\text{F1-Score} &= 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión:\n",
    "\n",
    "```math\n",
    "\\begin{align*}\n",
    "y &= \\text{valor real} \\\\\n",
    "\\hat{y} &= \\text{valor predicho} \\\\\n",
    "n &= \\text{número de muestras} \\\\\n",
    "\\bar{y} &= \\text{media de los valores reales} \\\\\n",
    "\\end{align*}\n",
    "```\n",
    "```math\n",
    "\\begin{align*}\n",
    "\\\\\n",
    "\\text{MSE (Error Cuadrático Medio)} &= \\frac{1}{n} \\sum (y - \\hat{y})^2 \\\\\n",
    "\\\\\n",
    "\\text{RMSE (Raíz del Error Cuadrático Medio)} &= \\sqrt{\\text{MSE}} \\\\\n",
    "\\\\\n",
    "\\text{MAE (Error Absoluto Medio)} &= \\frac{1}{n} \\sum |y - \\hat{y}| \\\\\n",
    "\\\\\n",
    "R^2 &= 1 - \\frac{\\sum (y - \\hat{y})^2}{\\sum (y - \\bar{y})^2}\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curvas ROC y AUC:\n",
    "\n",
    "```markdown\n",
    "TPR (True Positive Rate) = Recall = TP / (TP + FN)\n",
    "FPR (False Positive Rate) = FP / (FP + TN)\n",
    "\n",
    "AUC = Área bajo la curva ROC (integral de TPR respecto a FPR)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Loss (Cross-Entropy):\n",
    "\n",
    "```math\n",
    "\\text{Log Loss} = -\\frac{1}{n} \\sum \\left[ y \\log(p) + (1-y) \\log(1-p) \\right] \\\\ \n",
    "\\text{donde:}\\\\ \n",
    "y = \\text{etiqueta real} (0 \\text{ o } 1)\\\\ \n",
    "p = \\text{probabilidad predicha}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas para Regresión Avanzadas:\n",
    "\n",
    "```math\n",
    "\\begin{align*}\n",
    "\\text{MAPE (Error Porcentual Absoluto Medio)} &= \\frac{100}{n} \\sum \\frac{|y - \\hat{y}|}{|y|} \\\\\n",
    "\n",
    "\\text{MSLE (Error Cuadrático Logarítmico Medio)} &= \\frac{1}{n} \\sum \\left[ \\log(1 + y) - \\log(1 + \\hat{y}) \\right]^2 \\\\\n",
    "\n",
    "\\text{R² Ajustado} &= 1 - \\left[ \\frac{(1 - R²)(n-1)}{n-p-1} \\right] \\\\\n",
    "\\text{donde } p &= \\text{número de predictores}\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas para Clasificación Multiclase:\n",
    "\n",
    "```math\n",
    "\\begin{align*}\n",
    "\\text{Macro-Average Precision} &= \\frac{1}{k} \\sum (\\text{Precision por clase}) \\\\\n",
    "\\text{donde } k &= \\text{número de clases} \\\\\n",
    "\n",
    "\\text{Weighted-Average F1} &= \\frac{\\sum (\\text{F1 por clase} \\times \\text{soporte por clase})}{\\text{soporte total}}\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calcular_metricas(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula múltiples métricas de clasificación\n",
    "    \"\"\"\n",
    "    # Convertir a arrays numpy\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Total de muestras\n",
    "    n = len(y_true)\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Métricas básicas\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Error cuadrático medio\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'mse': mse,\n",
    "        'confusion_matrix': {\n",
    "            'TP': TP,\n",
    "            'TN': TN,\n",
    "            'FP': FP,\n",
    "            'FN': FN\n",
    "        }\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UP2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
